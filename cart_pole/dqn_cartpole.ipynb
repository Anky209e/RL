{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import json\n",
    "import os\n",
    "import pygame\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.utils.play import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/cart_pole_dqn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VfApproxModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network for Value Function Approximation\\n\n",
    "    Contains Three layers 4->10->10->2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=4,out_features=30)\n",
    "        self.layer_2 = nn.Linear(in_features=30,out_features=30)\n",
    "        self.layer_3 = nn.Linear(in_features=30,out_features=2) # 2 actions as output\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,features):\n",
    "        out = self.relu(self.layer_1(features))\n",
    "        out = self.relu(self.layer_2(out))\n",
    "        out = self.relu(self.layer_3(out))\n",
    "        # using softmax as action-selection policy\n",
    "        # out = torch.softmax(out,-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_policy = VfApproxModel().to(device)\n",
    "learning_policy = VfApproxModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights of learning policy\n",
    "target_policy.load_state_dict(learning_policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.tensor([0,0,4,0],dtype=torch.float32,device=device)\n",
    "learning_policy(state).max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_policy(TAU):\n",
    "        \"\"\"\n",
    "        Soft update of the target network's weights\\n\n",
    "        θ′ ← τ θ + (1 −τ )θ\n",
    "        \"\"\"\n",
    "        target_net_state_dict = target_policy.state_dict()\n",
    "        learning_state_dict = learning_policy.state_dict()\n",
    "        for key in learning_state_dict:\n",
    "                target_net_state_dict[key] = learning_state_dict[key]*TAU + target_net_state_dict[key] *(1-TAU)\n",
    "        \n",
    "        target_policy.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10*200\n",
    "GAMMA = 0.99 # discount factor\n",
    "\n",
    "ALPHA = 1e-4 # Learning rate\n",
    "EPSILON = 1 # e\n",
    "EPSILON_DECAY = 0.002\n",
    "MIN_EXP_RATE = 0.2\n",
    "MAX_EXP_RATE = 1\n",
    "\n",
    "REPLAY_LENGTH = 9000\n",
    "REPLAY_BATCH = 100\n",
    "TAU = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss().to(device)\n",
    "opt = torch.optim.SGD(learning_policy.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_update(state,new_state,reward,running):\n",
    "\n",
    "    state_action_value = learning_policy(state).max(0)[0]\n",
    "    with torch.no_grad():\n",
    "        expected_state_action_value = target_policy(new_state).max(0)[0]\n",
    "\n",
    "    expected_state_action_value = ((expected_state_action_value * GAMMA ) + reward) * bool(not(running))\n",
    "\n",
    "    loss = criterion(state_action_value,expected_state_action_value)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(learning_policy.parameters(), 100)\n",
    "    opt.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "        Replay Memory for string Experience\n",
    "    \"\"\"\n",
    "    def __init__(self,length,batch_size):\n",
    "        self.replay_memory = deque(maxlen=length)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add_experience(self,new_state,reward,running,state,action):\n",
    "        \"\"\"\n",
    "            Adds Experience into replay_memory\\n\n",
    "            new_state and state both are torch tensors\n",
    "        \"\"\"\n",
    "        self.replay_memory.append((new_state,reward,running,state,action))\n",
    "    \n",
    "    def train_on_replay(self):\n",
    "        \"\"\"\n",
    "            Training on Replay memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.replay_memory,self.batch_size)\n",
    "        \n",
    "        for new_state,reward,running,state,action in batch:\n",
    "\n",
    "            loss = q_update(state,new_state,reward,running)\n",
    "        \n",
    "        return loss # final loss of replay batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ReplayMemory(REPLAY_LENGTH,batch_size=REPLAY_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "for episode in range(EPISODES):\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    state,info = env.reset()\n",
    "    reward_per_episode = 0\n",
    "    state = torch.tensor(state,dtype=torch.float32,requires_grad=True,device=device)\n",
    "    while not (terminated or truncated):\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        # E-greedy for exploration vs exploitation\n",
    "        if exploration_rate_threshold > EPSILON:\n",
    "            with torch.no_grad():\n",
    "                action = learning_policy(state).max(0)[1].item()\n",
    "        else:\n",
    "            action = random.randint(0,1)\n",
    "        \n",
    "        new_state,reward,terminated,truncated,info = env.step(action)\n",
    "        \n",
    "        new_state = torch.tensor(new_state,dtype=torch.float32,requires_grad=True,device=device)\n",
    "        running = terminated or truncated\n",
    "\n",
    "        replay_memory.add_experience(new_state,reward,running,state,action)\n",
    "        \n",
    "        loss = q_update(state,new_state,reward,running)\n",
    "\n",
    "        state = new_state\n",
    "        reward_per_episode += reward\n",
    "    \n",
    "    print(f\"-------Episode:[{episode+1}/{EPISODES}]--------\")\n",
    "    print(\"Reward per Episode: \",reward_per_episode)\n",
    "    print(\"Loss per Episode\",loss.item())\n",
    "    \n",
    "    writer.add_scalar(\"Exploration Rate\",EPSILON,episode)\n",
    "    EPSILON = (MAX_EXP_RATE-MIN_EXP_RATE) * np.exp(-EPSILON_DECAY*episode) + MIN_EXP_RATE\n",
    "    writer.add_scalar(\"Reward per Episode\",reward_per_episode,episode)\n",
    "    writer.add_scalar(\"Loss per Episode\",loss.item(),episode)\n",
    "    \n",
    "    if len(replay_memory.replay_memory) > REPLAY_BATCH:\n",
    "        replay_loss = replay_memory.train_on_replay()\n",
    "        writer.add_scalar(\"Replay Loss\",replay_loss.item(),episode)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(\"[Updating Policy]\")\n",
    "        update_target_policy(TAU)\n",
    "        \n",
    "\n",
    "\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.display.quit() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
