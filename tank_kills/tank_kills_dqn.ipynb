{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 19:35:07.114010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 19:35:08.321076: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-10 19:35:08.321170: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-10 19:35:08.321178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tank_kills_v3 import TankKills\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/dqn_tank_kills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VfApproxModel(\n",
       "  (layer_1): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (layer_3): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VfApproxModel(nn.Module):\n",
    "    \"\"\"Neural Network for Value Function Approximation\\n\n",
    "    Contains Three layers 4->10->10->4\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input will be [player_x,player_y,enemy_x,enemy_y]\n",
    "        # NOTE: Later on we will provide s,a,r,s,a\n",
    "        self.layer_1 = nn.Linear(in_features=4,out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10,out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10,out_features=4) # 4 actions as output\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,features):\n",
    "        out = self.relu(self.layer_1(features))\n",
    "        out = self.relu(self.layer_2(out))\n",
    "        out = self.layer_3(out)\n",
    "        # using softmax as action-selection policy\n",
    "        # out = torch.softmax(out,-1)\n",
    "        return out\n",
    "\n",
    "value_function = VfApproxModel()\n",
    "value_function.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" style=\"height:400px;width:50%;float:left;\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg\" style=\"height:400px;width:50%;float:right;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Main Agent Class Contiaining replay Memory and all learning params\"\"\"\n",
    "    def __init__(self,replay_length,learning_rate,epsilon,max_epsilon,min_epsilon,epsilon_decay,gamma,action_size,value_function):\n",
    "        self.replay_memory = deque(maxlen=replay_length)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.value_function = value_function\n",
    "        self.loss_fn = nn.SmoothL1Loss().to(device)\n",
    "        self.opt  = torch.optim.AdamW(value_function.parameters(), lr=learning_rate, amsgrad=True)\n",
    "        self.epsilon_list = []\n",
    "        self.losses =  []\n",
    "\n",
    "    def add_experience(self,new_state,reward,running,state,action):\n",
    "        \"\"\"\n",
    "            Adds Experience into replay_memory\n",
    "            new_state = [new_player_x,new_player_y,new_enemy_x,new_enemy_y]\n",
    "            state = [player_x,player_y,enemy_x,enemy_y]\n",
    "            new_state ans state both are torch tensors\n",
    "        \"\"\"\n",
    "        self.replay_memory.append((new_state,reward,running,state,action))\n",
    "    \n",
    "    \n",
    "    def action(self,state):\n",
    "        \"\"\"For Taking action using e-greedy\"\"\"\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            return np.random.randint(0,3)\n",
    "        out = self.value_function(state)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        return np.argmax(out)\n",
    "    \n",
    "    \n",
    "    def greedy_action(self,state):\n",
    "        \"\"\"Predicts a action greedily only using value function\"\"\"\n",
    "\n",
    "        out = self.value_function(state)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        return np.argmax(out)\n",
    "    \n",
    "    \n",
    "    def replay(self,batch_size,episode):\n",
    "        \"\"\"Learning From Experience Replay\"\"\"\n",
    "        \n",
    "        # batch size is how many batches you want to iterate to learn over\n",
    "        batch = random.sample(self.replay_memory,batch_size)\n",
    "        # NOTE: new_state and state should be torch tensor \n",
    "        for new_state,reward,running,state,action in batch:\n",
    "            target = reward\n",
    "            \n",
    "            if running:\n",
    "                \n",
    "                target = self.value_function(new_state)\n",
    "                # target = target.cpu().detach().numpy()\n",
    "                target = reward + torch.mul((self.gamma*target),1-bool( not running))\n",
    "                # target = torch.tensor(target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    former_target = self.value_function(state)\n",
    "                    former_target = former_target.cpu().detach().numpy()\n",
    "                    former_target = np.amax(former_target)\n",
    "                    former_target = torch.tensor(former_target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "\n",
    "                loss = self.loss_fn(target,former_target)\n",
    "                \n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "            \n",
    "            self.epsilon_list.append(self.epsilon)\n",
    "        return loss.item()\n",
    "\n",
    "    def save_weights(self,path):\n",
    "        \"\"\"Save Current State weights\"\"\"\n",
    "        torch.save(self.value_function.state_dict(),path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function.load_state_dict(torch.load(\"tank_kills/saved_weights/last_episode_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = [\"up\",\"right\",\"down\",\"left\"]\n",
    "num_episodes = 50\n",
    "\n",
    "learning_rate = 0.0005 # alpha\n",
    "discount_factor = 0.80 # gamma\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.04\n",
    "exploration_decay_rate = 0.005\n",
    "\n",
    "replay_length = 5000# Replay memory (D) or length (N)\n",
    "batch_size = 60 # Batch size to train on replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    replay_length=replay_length,\n",
    "    learning_rate=learning_rate,\n",
    "    epsilon=exploration_rate,\n",
    "    max_epsilon=max_exploration_rate,\n",
    "    min_epsilon=min_exploration_rate,\n",
    "    epsilon_decay=exploration_decay_rate,\n",
    "    gamma=discount_factor,\n",
    "    action_size=4,\n",
    "    value_function=value_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Episode: [1/50]-----\n",
      "Reward:-2095\n",
      "Score:0\n",
      "Actions Taken:2046\n",
      "Loss:44.51579284667969\n",
      "-----Episode: [2/50]-----\n",
      "Reward:-2674\n",
      "Score:1\n",
      "Actions Taken:2636\n",
      "Loss:44.430511474609375\n",
      "-----Episode: [3/50]-----\n",
      "Reward:-3519\n",
      "Score:2\n",
      "Actions Taken:3492\n",
      "Loss:44.520591735839844\n",
      "-----Episode: [4/50]-----\n",
      "Reward:-2095\n",
      "Score:0\n",
      "Actions Taken:2046\n",
      "Loss:44.50000762939453\n",
      "-----Episode: [5/50]-----\n",
      "Reward:-2050\n",
      "Score:0\n",
      "Actions Taken:2001\n",
      "Loss:44.49191665649414\n",
      "-----Episode: [6/50]-----\n",
      "Reward:-4034\n",
      "Score:1\n",
      "Actions Taken:3996\n",
      "Loss:44.498191833496094\n",
      "-----Episode: [7/50]-----\n",
      "Reward:-3079\n",
      "Score:1\n",
      "Actions Taken:3041\n",
      "Loss:44.51842498779297\n",
      "-----Episode: [8/50]-----\n",
      "Reward:-4479\n",
      "Score:2\n",
      "Actions Taken:4452\n",
      "Loss:44.48652648925781\n",
      "-----Episode: [9/50]-----\n",
      "Reward:-2050\n",
      "Score:0\n",
      "Actions Taken:2001\n",
      "Loss:44.47859191894531\n",
      "-----Episode: [10/50]-----\n",
      "Reward:-2075\n",
      "Score:0\n",
      "Actions Taken:2026\n",
      "Loss:44.499290466308594\n",
      "-----Episode: [11/50]-----\n",
      "Reward:-2135\n",
      "Score:0\n",
      "Actions Taken:2086\n",
      "Loss:44.53248977661133\n",
      "-----Episode: [12/50]-----\n",
      "Reward:-4065\n",
      "Score:2\n",
      "Actions Taken:4038\n",
      "Loss:44.419158935546875\n",
      "-----Episode: [13/50]-----\n",
      "Reward:-2135\n",
      "Score:0\n",
      "Actions Taken:2086\n",
      "Loss:44.47721862792969\n",
      "-----Episode: [14/50]-----\n",
      "Reward:-3415\n",
      "Score:1\n",
      "Actions Taken:3377\n",
      "Loss:44.50169372558594\n",
      "-----Episode: [15/50]-----\n",
      "Reward:-2110\n",
      "Score:0\n",
      "Actions Taken:2061\n",
      "Loss:44.436988830566406\n",
      "-----Episode: [16/50]-----\n",
      "Reward:-5045\n",
      "Score:2\n",
      "Actions Taken:5018\n",
      "Loss:44.482200622558594\n",
      "-----Episode: [17/50]-----\n",
      "Reward:-2070\n",
      "Score:0\n",
      "Actions Taken:2021\n",
      "Loss:44.47905731201172\n",
      "-----Episode: [18/50]-----\n",
      "Reward:-2707\n",
      "Score:1\n",
      "Actions Taken:2669\n",
      "Loss:44.487239837646484\n",
      "-----Episode: [19/50]-----\n",
      "Reward:-2055\n",
      "Score:0\n",
      "Actions Taken:2006\n",
      "Loss:44.506385803222656\n",
      "-----Episode: [20/50]-----\n",
      "Reward:-2145\n",
      "Score:0\n",
      "Actions Taken:2096\n",
      "Loss:44.46986389160156\n",
      "-----Episode: [21/50]-----\n",
      "Reward:-2145\n",
      "Score:0\n",
      "Actions Taken:2096\n",
      "Loss:44.494354248046875\n",
      "-----Episode: [22/50]-----\n",
      "Reward:-2861\n",
      "Score:2\n",
      "Actions Taken:2834\n",
      "Loss:44.510963439941406\n",
      "-----Episode: [23/50]-----\n",
      "Reward:-2060\n",
      "Score:0\n",
      "Actions Taken:2011\n",
      "Loss:44.49739456176758\n",
      "-----Episode: [24/50]-----\n",
      "Reward:-2100\n",
      "Score:0\n",
      "Actions Taken:2051\n",
      "Loss:44.491119384765625\n",
      "-----Episode: [25/50]-----\n",
      "Reward:-2065\n",
      "Score:0\n",
      "Actions Taken:2016\n",
      "Loss:44.509525299072266\n",
      "-----Episode: [26/50]-----\n",
      "Reward:-2130\n",
      "Score:0\n",
      "Actions Taken:2081\n",
      "Loss:44.480262756347656\n",
      "-----Episode: [27/50]-----\n",
      "Reward:-2105\n",
      "Score:0\n",
      "Actions Taken:2056\n",
      "Loss:44.504798889160156\n",
      "-----Episode: [28/50]-----\n",
      "Reward:-2135\n",
      "Score:0\n",
      "Actions Taken:2086\n",
      "Loss:44.45209884643555\n",
      "-----Episode: [29/50]-----\n",
      "Reward:-2050\n",
      "Score:0\n",
      "Actions Taken:2001\n",
      "Loss:44.45939636230469\n",
      "-----Episode: [30/50]-----\n",
      "Reward:-2075\n",
      "Score:0\n",
      "Actions Taken:2026\n",
      "Loss:44.46394348144531\n",
      "-----Episode: [31/50]-----\n",
      "Reward:-2090\n",
      "Score:0\n",
      "Actions Taken:2041\n",
      "Loss:44.477081298828125\n",
      "-----Episode: [32/50]-----\n",
      "Reward:-3232\n",
      "Score:1\n",
      "Actions Taken:3194\n",
      "Loss:44.47486114501953\n",
      "-----Episode: [33/50]-----\n",
      "Reward:-2085\n",
      "Score:0\n",
      "Actions Taken:2036\n",
      "Loss:44.48511505126953\n",
      "-----Episode: [34/50]-----\n",
      "Reward:-2145\n",
      "Score:0\n",
      "Actions Taken:2096\n",
      "Loss:44.46935272216797\n",
      "-----Episode: [35/50]-----\n",
      "Reward:-2115\n",
      "Score:0\n",
      "Actions Taken:2066\n",
      "Loss:44.48119354248047\n",
      "-----Episode: [36/50]-----\n",
      "Reward:-3440\n",
      "Score:1\n",
      "Actions Taken:3402\n",
      "Loss:44.508750915527344\n",
      "-----Episode: [37/50]-----\n",
      "Reward:-3076\n",
      "Score:1\n",
      "Actions Taken:3038\n",
      "Loss:44.41537094116211\n",
      "-----Episode: [38/50]-----\n",
      "Reward:-2080\n",
      "Score:0\n",
      "Actions Taken:2031\n",
      "Loss:44.4393310546875\n",
      "-----Episode: [39/50]-----\n",
      "Reward:-4389\n",
      "Score:2\n",
      "Actions Taken:4362\n",
      "Loss:44.410850524902344\n",
      "-----Episode: [40/50]-----\n",
      "Reward:-2090\n",
      "Score:0\n",
      "Actions Taken:2041\n",
      "Loss:44.48823547363281\n",
      "-----Episode: [41/50]-----\n",
      "Reward:-2120\n",
      "Score:0\n",
      "Actions Taken:2071\n",
      "Loss:44.483116149902344\n",
      "-----Episode: [42/50]-----\n",
      "Reward:-5369\n",
      "Score:3\n",
      "Actions Taken:5353\n",
      "Loss:44.4826545715332\n",
      "-----Episode: [43/50]-----\n",
      "Reward:-2065\n",
      "Score:0\n",
      "Actions Taken:2016\n",
      "Loss:44.49565124511719\n",
      "-----Episode: [44/50]-----\n",
      "Reward:-3357\n",
      "Score:2\n",
      "Actions Taken:3330\n",
      "Loss:44.46197509765625\n",
      "-----Episode: [45/50]-----\n",
      "Reward:-2105\n",
      "Score:0\n",
      "Actions Taken:2056\n",
      "Loss:44.463653564453125\n",
      "-----Episode: [46/50]-----\n",
      "Reward:-2120\n",
      "Score:0\n",
      "Actions Taken:2071\n",
      "Loss:44.47124481201172\n",
      "-----Episode: [47/50]-----\n",
      "Reward:-2070\n",
      "Score:0\n",
      "Actions Taken:2021\n",
      "Loss:44.475521087646484\n",
      "-----Episode: [48/50]-----\n",
      "Reward:-2973\n",
      "Score:1\n",
      "Actions Taken:2935\n",
      "Loss:44.50053024291992\n",
      "-----Episode: [49/50]-----\n",
      "Reward:-2060\n",
      "Score:0\n",
      "Actions Taken:2011\n",
      "Loss:44.47661590576172\n",
      "-----Episode: [50/50]-----\n",
      "Reward:-2110\n",
      "Score:0\n",
      "Actions Taken:2061\n",
      "Loss:44.468624114990234\n"
     ]
    }
   ],
   "source": [
    "losses_per_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    old_score = 0\n",
    "    print(f\"-----Episode: [{episode+1}/{num_episodes}]-----\")\n",
    "    env = TankKills(600,600)\n",
    "    state = [300,400,0,0]\n",
    "    state = torch.tensor(state,dtype=torch.float32,device=device,requires_grad=True)\n",
    "    running = True\n",
    "    losses_per_action = []\n",
    "    actions_taken_per_episode = 0\n",
    "    rewards_per_episode = 0\n",
    "    steps = 0\n",
    "    while running:\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > agent.epsilon:\n",
    "            action = agent.greedy_action(state)\n",
    "        else:\n",
    "            action = random.randint(0,3)\n",
    "        \n",
    "        running,reward,score,pp,ep = env.play(action=all_actions[action])\n",
    "\n",
    "        new_state = [pp[0],pp[1],ep[0],ep[1]]\n",
    "        new_state = torch.tensor(new_state,dtype=torch.float32,device=device,requires_grad=True)\n",
    "\n",
    "        target = agent.value_function(new_state)\n",
    "        # target = target.cpu().detach().numpy()\n",
    "        # target = reward + agent.gamma * np.amax(target)\n",
    "        # target = reward + (agent.gamma * target)\n",
    "        target = reward + torch.mul((agent.gamma*target),1-bool( not running))\n",
    "        # target = torch.tensor(target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "\n",
    "        \n",
    "        former_target = agent.value_function(state)\n",
    "            # former_target = former_target.cpu().detach().numpy()\n",
    "            # former_target = np.amax(former_target)\n",
    "            # former_target = torch.tensor(former_target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "        \n",
    "        \n",
    "        # print(target.shape,former_target.shape)\n",
    "        loss = agent.loss_fn(target,former_target)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        # print(target.grad,former_target.grad)\n",
    "        agent.opt.step()\n",
    "        agent.opt.zero_grad()\n",
    "        torch.nn.utils.clip_grad_value_(agent.value_function.parameters(), 100)\n",
    "        agent.add_experience(new_state,reward,running,state,action)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        rewards_per_episode+=reward\n",
    "        actions_taken_per_episode += 1\n",
    "        losses_per_action.append(loss.item())\n",
    "        writer.add_scalar(\"Loss per action\",loss.item(),actions_taken_per_episode)\n",
    "    \n",
    "    agent.epsilon = (agent.max_epsilon-agent.min_epsilon) * np.exp(-agent.epsilon_decay*episode) + agent.min_epsilon\n",
    "    pygame.display.quit()\n",
    "    \n",
    "    if score > old_score:\n",
    "        agent.save_weights(f\"saved_weights/agent_{score}.pth\")\n",
    "    old_score = score\n",
    "    \n",
    "    writer.add_scalar(\"Reward per Episode\",rewards_per_episode,episode)\n",
    "    writer.add_scalar(\"Actions Taken per Episode\",actions_taken_per_episode,episode)\n",
    "    writer.add_scalar(\"Score\",score,episode)\n",
    "    writer.add_scalar(\"Loss per Episode\",loss.item(),episode)\n",
    "    writer.add_scalar(\"Epsilon per episode\",agent.epsilon,episode)\n",
    "    \n",
    "\n",
    "    print(f\"Reward:{rewards_per_episode}\")\n",
    "    print(f\"Score:{score}\")\n",
    "    print(f\"Actions Taken:{actions_taken_per_episode}\")\n",
    "    print(f\"Loss:{loss}\")\n",
    "\n",
    "\n",
    "    # if len(agent.replay_memory) > batch_size:\n",
    "    #     replay_loss = agent.replay(batch_size,episode)\n",
    "    #     writer.add_scalar(\"Replay Loss\",replay_loss,episode)\n",
    "    #     print(\"[Trained on replay]\\n\")\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "agent.save_weights(\"saved_weights/last_episode_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.replay_memory[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = agent.replay_memory[20][0]\n",
    "state = agent.replay_memory[20][3]\n",
    "reward = agent.replay_memory[20][1]\n",
    "\n",
    "target = agent.value_function(state)\n",
    "target = reward + torch.mul((agent.gamma*target),1)\n",
    "# target = target.cpu().detach().numpy()\n",
    "# target = reward + (agent.gamma * target)\n",
    "# target = torch.tensor(target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "# with torch.no_grad():\n",
    "former_target = agent.value_function(state)\n",
    "# former_target = former_target.cpu().detach().numpy()\n",
    "# former_target = np.amax(former_target)\n",
    "# former_target = torch.tensor(former_target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "# print(target.grad,former_target.grad)\n",
    "\n",
    "\n",
    "print(target,former_target)\n",
    "\n",
    "loss = agent.loss_fn(target,former_target)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(target.grad,former_target.grad)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.display.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
