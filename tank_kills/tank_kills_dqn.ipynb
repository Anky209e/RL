{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tank_kills_v3 import TankKills\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/dqn_tank_kills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = [\"up\",\"right\",\"down\",\"left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2\n",
    "\n",
    "learning_rate = 0.1 # alpha\n",
    "discount_factor = 0.90 # gamma\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.007\n",
    "\n",
    "replay_memory = deque(maxlen=500)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VfApproxModel(nn.Module):\n",
    "    \"\"\"Neural Network for Value Function Approximation\\n\n",
    "    Contains Three layers 4->10->10->4\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input will be [player_x,player_y,enemy_x,enemy_y]\n",
    "        # NOTE: Later on we will provide s,a,r,s,a\n",
    "        self.layer_1 = nn.Linear(in_features=4,out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10,out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10,out_features=4) # 4 actions as output\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,features):\n",
    "        out = self.relu(self.layer_1(features))\n",
    "        out = self.relu(self.layer_2(out))\n",
    "        out = self.relu(self.layer_3(out))\n",
    "        # using softmax as action-selection policy\n",
    "        out = torch.softmax(out,-1)\n",
    "        return out\n",
    "\n",
    "value_function = VfApproxModel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" style=\"height:400px;width:50%;float:left;\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg\" style=\"height:400px;width:50%;float:right;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Main Agent Class Contiaining replay Memory and all learning params\"\"\"\n",
    "    def __init__(self,replay_length,learning_rate,epsilon,max_epsilon,min_epsilon,epsilon_decay,gamma,action_size,value_function):\n",
    "        self.replay_memory = deque(maxlen=replay_length)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.value_function = value_function\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.opt  = torch.optim.AdamW(value_function.parameters(), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "\n",
    "    def add_experience(self,new_state,reward,running,state,action):\n",
    "        \"\"\"\n",
    "            Adds Experience into replay_memory\n",
    "            new_state = [new_player_x,new_player_y,new_enemy_x,new_enemy_y]\n",
    "            state = [player_x,player_y,enemy_x,enemy_y]\n",
    "            new_state ans state both are torch tensors\n",
    "        \"\"\"\n",
    "        self.replay_memory.append((new_state,reward,running,state,action))\n",
    "    \n",
    "    \n",
    "    def action(self,state):\n",
    "        \"\"\"For Taking action using e-greedy\"\"\"\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            return np.random.randint(0,3)\n",
    "        out = self.val(state)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        return np.argmax(out)\n",
    "    \n",
    "    \n",
    "    def greedy_action(self,state):\n",
    "        \"\"\"Predicts a action greedily only using value function\"\"\"\n",
    "\n",
    "        out = self.val(state)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        return np.argmax(out)\n",
    "    \n",
    "    \n",
    "    def replay(self,batch_size):\n",
    "        \"\"\"Learning From Experience Replay\"\"\"\n",
    "        \n",
    "        # batch size is how many batches you want to iterate to learn over\n",
    "        batch = random.sample(self.replay_memory,batch_size)\n",
    "        # NOTE: new_state and state should be torch tensor \n",
    "        for new_state,reward,running,state,action in batch:\n",
    "            target = reward\n",
    "            \n",
    "            if running:\n",
    "                with torch.no_grad():\n",
    "                    next_state_q_value = self.value_function(new_state)\n",
    "                    next_state_q_value = next_state_q_value.cpu().detach().numpy()\n",
    "                    target = reward + self.gamma * np.amax(next_state_q_value)\n",
    "                    target = torch.tensor(target)\n",
    "\n",
    "                    former_target = self.value_function(state)\n",
    "                    \n",
    "                loss = self.loss_fn(target,former_target.unsqueeze(1))\n",
    "                self.opt.zero_grad()\n",
    "                loss.backwards()\n",
    "                self.opt.step()\n",
    "                 \n",
    "\n",
    "            pass\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
