{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anky/Github/RL/.venv/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from tank_kills_v3 import TankKills\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/dqn_tank_kills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VfApproxModel(\n",
       "  (layer_1): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (layer_3): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VfApproxModel(nn.Module):\n",
    "    \"\"\"Neural Network for Value Function Approximation\\n\n",
    "    Contains Three layers 4->10->10->4\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input will be [player_x,player_y,enemy_x,enemy_y]\n",
    "        # NOTE: Later on we will provide s,a,r,s,a\n",
    "        self.layer_1 = nn.Linear(in_features=4,out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10,out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10,out_features=4) # 4 actions as output\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,features):\n",
    "        out = self.relu(self.layer_1(features))\n",
    "        out = self.relu(self.layer_2(out))\n",
    "        out = self.layer_3(out)\n",
    "        # using softmax as action-selection policy\n",
    "        # out = torch.softmax(out,-1)\n",
    "        return out\n",
    "\n",
    "value_function = VfApproxModel()\n",
    "value_function.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" style=\"height:400px;width:50%;float:left;\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg\" style=\"height:400px;width:50%;float:right;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Main Agent Class Contiaining replay Memory and all learning params\"\"\"\n",
    "    def __init__(self,replay_length,learning_rate,epsilon,max_epsilon,min_epsilon,epsilon_decay,gamma,action_size,value_function):\n",
    "        self.replay_memory = deque(maxlen=replay_length)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.value_function = value_function\n",
    "        self.loss_fn = nn.SmoothL1Loss().to(device)\n",
    "        self.opt  = torch.optim.AdamW(value_function.parameters(), lr=learning_rate, amsgrad=True)\n",
    "        self.epsilon_list = []\n",
    "        self.losses =  []\n",
    "\n",
    "    def add_experience(self,new_state,reward,running,state,action):\n",
    "        \"\"\"\n",
    "            Adds Experience into replay_memory\n",
    "            new_state = [new_player_x,new_player_y,new_enemy_x,new_enemy_y]\n",
    "            state = [player_x,player_y,enemy_x,enemy_y]\n",
    "            new_state ans state both are torch tensors\n",
    "        \"\"\"\n",
    "        self.replay_memory.append((new_state,reward,running,state,action))\n",
    "    \n",
    "    \n",
    "    def action(self,state):\n",
    "        \"\"\"For Taking action using e-greedy\"\"\"\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0,self.action_size)\n",
    "        out = self.value_function(state)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        return np.argmax(out)\n",
    "    \n",
    "    \n",
    "    def greedy_action(self,state):\n",
    "        \"\"\"Predicts a action greedily only using value function\"\"\"\n",
    "\n",
    "        out = self.value_function(state)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        return np.argmax(out)\n",
    "    \n",
    "    \n",
    "    def replay(self, batch_size, episode):\n",
    "        batch = random.sample(self.replay_memory, batch_size)\n",
    "        losses = []\n",
    "        for new_state, reward, running, state, action in batch:\n",
    "            # Predict Q(s, a) for current state\n",
    "            q_values = self.value_function(state)\n",
    "            q_value = q_values[action]\n",
    "\n",
    "            # Predict Q(s', a') for next state\n",
    "            with torch.no_grad():\n",
    "                next_q_values = self.value_function(new_state)\n",
    "                max_next_q = torch.max(next_q_values)\n",
    "                target = reward + self.gamma * max_next_q * float(running)\n",
    "\n",
    "            loss = self.loss_fn(q_value, target)\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            losses.append(loss.item())\n",
    "        return np.mean(losses) if losses else 0.0\n",
    "\n",
    "    def save_weights(self,path):\n",
    "        \"\"\"Save Current State weights\"\"\"\n",
    "        torch.save(self.value_function.state_dict(),path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_function.load_state_dict(torch.load(\"saved_weights/last_episode_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = [\"up\",\"right\",\"down\",\"left\"]\n",
    "num_episodes = 20\n",
    "\n",
    "learning_rate = 0.1 # alpha\n",
    "discount_factor = 0.80 # gamma\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.04\n",
    "exploration_decay_rate = 0.005\n",
    "\n",
    "replay_length = 5000# Replay memory (D) or length (N)\n",
    "batch_size = 300 # Batch size to train on replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    replay_length=replay_length,\n",
    "    learning_rate=learning_rate,\n",
    "    epsilon=exploration_rate,\n",
    "    max_epsilon=max_exploration_rate,\n",
    "    min_epsilon=min_exploration_rate,\n",
    "    epsilon_decay=exploration_decay_rate,\n",
    "    gamma=discount_factor,\n",
    "    action_size=4,\n",
    "    value_function=value_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Episode: [1/20]-----\n",
      "Reward:-1991\n",
      "Score:0\n",
      "Actions Taken:2071\n",
      "Loss:25.883682250976562\n",
      "-----Episode: [2/20]-----\n",
      "Reward:-1010\n",
      "Score:0\n",
      "Actions Taken:2071\n",
      "Loss:25.458993911743164\n",
      "-----Episode: [3/20]-----\n",
      "Reward:-1661\n",
      "Score:0\n",
      "Actions Taken:2016\n",
      "Loss:26.99167251586914\n",
      "-----Episode: [4/20]-----\n",
      "Reward:-2391\n",
      "Score:3\n",
      "Actions Taken:5144\n",
      "Loss:27.546947479248047\n",
      "-----Episode: [5/20]-----\n",
      "Reward:-1051\n",
      "Score:1\n",
      "Actions Taken:2608\n",
      "Loss:28.0220947265625\n",
      "-----Episode: [6/20]-----\n",
      "Reward:-3156\n",
      "Score:8\n",
      "Actions Taken:9097\n",
      "Loss:28.385765075683594\n",
      "-----Episode: [7/20]-----\n",
      "Reward:-2232\n",
      "Score:1\n",
      "Actions Taken:3658\n",
      "Loss:28.892274856567383\n",
      "-----Episode: [8/20]-----\n",
      "Reward:-1608\n",
      "Score:1\n",
      "Actions Taken:3869\n",
      "Loss:28.4791259765625\n",
      "-----Episode: [9/20]-----\n",
      "Reward:-1685\n",
      "Score:0\n",
      "Actions Taken:2046\n",
      "Loss:28.572784423828125\n",
      "-----Episode: [10/20]-----\n",
      "Reward:-1626\n",
      "Score:1\n",
      "Actions Taken:3573\n",
      "Loss:27.686756134033203\n",
      "-----Episode: [11/20]-----\n",
      "Reward:-2319\n",
      "Score:3\n",
      "Actions Taken:5337\n",
      "Loss:28.359821319580078\n",
      "-----Episode: [12/20]-----\n",
      "Reward:-1592\n",
      "Score:0\n",
      "Actions Taken:2001\n",
      "Loss:28.854602813720703\n",
      "-----Episode: [13/20]-----\n",
      "Reward:-1367\n",
      "Score:0\n",
      "Actions Taken:2006\n",
      "Loss:28.28630828857422\n",
      "-----Episode: [14/20]-----\n",
      "Reward:-1379\n",
      "Score:0\n",
      "Actions Taken:2006\n",
      "Loss:26.311458587646484\n",
      "-----Episode: [15/20]-----\n",
      "Reward:-2448\n",
      "Score:2\n",
      "Actions Taken:5328\n",
      "Loss:27.276559829711914\n",
      "-----Episode: [16/20]-----\n",
      "Reward:-1977\n",
      "Score:1\n",
      "Actions Taken:2865\n",
      "Loss:28.188175201416016\n",
      "-----Episode: [17/20]-----\n",
      "Reward:-2846\n",
      "Score:2\n",
      "Actions Taken:5372\n",
      "Loss:28.297243118286133\n",
      "-----Episode: [18/20]-----\n",
      "Reward:-2011\n",
      "Score:0\n",
      "Actions Taken:2076\n",
      "Loss:28.030054092407227\n",
      "-----Episode: [19/20]-----\n",
      "Reward:-2042\n",
      "Score:0\n",
      "Actions Taken:2021\n",
      "Loss:26.848283767700195\n",
      "-----Episode: [20/20]-----\n",
      "Reward:-1441\n",
      "Score:0\n",
      "Actions Taken:2001\n",
      "Loss:25.506555557250977\n"
     ]
    }
   ],
   "source": [
    "losses_per_episode = []\n",
    "for episode in range(num_episodes):\n",
    "    old_score = 0\n",
    "    print(f\"-----Episode: [{episode+1}/{num_episodes}]-----\")\n",
    "    env = TankKills(600,600)\n",
    "    state = [300/600,400/600,0/600,0/600]\n",
    "    state = torch.tensor(state,dtype=torch.float32,device=device,requires_grad=True)\n",
    "    running = True\n",
    "    losses_per_action = []\n",
    "    actions_taken_per_episode = 0\n",
    "    rewards_per_episode = 0\n",
    "    steps = 0\n",
    "    while running:\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        action = agent.action(state)\n",
    "\n",
    "        running,reward,score,pp,ep = env.play(action=all_actions[action])\n",
    "\n",
    "        new_state = [pp[0]/600,pp[1]/600,ep[0]/600,ep[1]/600]\n",
    "        new_state = torch.tensor(new_state,dtype=torch.float32,device=device,requires_grad=True)\n",
    "\n",
    "        target = agent.value_function(new_state)\n",
    "        \n",
    "        # target = reward + torch.mul((agent.gamma*target),1-bool( not running))\n",
    "        target = target.cpu().detach().numpy()\n",
    "        target = reward + (agent.gamma * np.amax(target))*(1-bool( not running))\n",
    "        target = torch.tensor(target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "        \n",
    "        former_target = agent.value_function(state)\n",
    "\n",
    "        former_target = former_target.cpu().detach().numpy()\n",
    "        former_target = np.amax(former_target)\n",
    "        former_target = torch.tensor(former_target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "        \n",
    "        loss = agent.loss_fn(target,former_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        # print(target.grad,former_target.grad)\n",
    "        agent.opt.step()\n",
    "        agent.opt.zero_grad()\n",
    "        # torch.nn.utils.clip_grad_value_(agent.value_function.parameters(), 100)\n",
    "        agent.add_experience(new_state,reward,running,state,action)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        rewards_per_episode+=reward\n",
    "        actions_taken_per_episode += 1\n",
    "        losses_per_action.append(loss.item())\n",
    "        writer.add_scalar(\"Loss per action\",loss.item(),actions_taken_per_episode)\n",
    "        \n",
    "    agent.epsilon = (agent.max_epsilon-agent.min_epsilon) * np.exp(-agent.epsilon_decay*episode) + agent.min_epsilon\n",
    "    pygame.display.quit()\n",
    "\n",
    "    if len(agent.replay_memory) > batch_size:\n",
    "        replay_loss = agent.replay(batch_size, episode)\n",
    "        writer.add_scalar(\"Replay Loss\", replay_loss, episode)\n",
    "    \n",
    "    if score > old_score:\n",
    "        agent.save_weights(f\"saved_weights/agent_{score}.pth\")\n",
    "    old_score = score\n",
    "    \n",
    "    writer.add_scalar(\"Reward per Episode\",rewards_per_episode,episode)\n",
    "    writer.add_scalar(\"Actions Taken per Episode\",actions_taken_per_episode,episode)\n",
    "    writer.add_scalar(\"Score\",score,episode)\n",
    "    writer.add_scalar(\"Loss per Episode\",loss.item(),episode)\n",
    "    writer.add_scalar(\"Epsilon per episode\",agent.epsilon,episode)\n",
    "    \n",
    "\n",
    "    print(f\"Reward:{rewards_per_episode}\")\n",
    "    print(f\"Score:{score}\")\n",
    "    print(f\"Actions Taken:{actions_taken_per_episode}\")\n",
    "    print(f\"Loss:{loss}\")\n",
    "\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "agent.save_weights(\"saved_weights/last_episode_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in agent.replay_memory:\n",
    "    if i[1] == 50:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.0044, -4.0493, -4.0410, -4.0646], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>) tensor([-3.7555, -3.8116, -3.8013, -3.8307], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "None None\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<SmoothL1LossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40849/3544859601.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(target.grad,former_target.grad)\n"
     ]
    }
   ],
   "source": [
    "new_state = agent.replay_memory[20][0]\n",
    "state = agent.replay_memory[20][3]\n",
    "reward = agent.replay_memory[20][1]\n",
    "\n",
    "target = agent.value_function(state)\n",
    "target = reward + torch.mul((agent.gamma*target),1)\n",
    "# target = target.cpu().detach().numpy()\n",
    "# target = reward + (agent.gamma * target)\n",
    "# target = torch.tensor(target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "# with torch.no_grad():\n",
    "former_target = agent.value_function(state)\n",
    "# former_target = former_target.cpu().detach().numpy()\n",
    "# former_target = np.amax(former_target)\n",
    "# former_target = torch.tensor(former_target,dtype=torch.float32,device=device,requires_grad=True)\n",
    "# print(target.grad,former_target.grad)\n",
    "\n",
    "\n",
    "print(target,former_target)\n",
    "\n",
    "loss = agent.loss_fn(target,former_target)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(target.grad,former_target.grad)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.display.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
